version: '3.8'

services:
  # GPU-enabled service with official backend (default)
  qwen3-tts-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: qwen3-tts-api
    network_mode: host
    ports:
      - "19160:19160"
    environment:
      - HOST=0.0.0.0
      - PORT=19160
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-Base
      - TTS_MULTI_MODEL=true
      - TTS_MULTI_GPU=true
      - TTS_GPU_DEVICE_IDS=all
      - TTS_SPEECH_MODEL_NAME=none
      - TTS_BASE_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-Base
      - TTS_VOICE_DESIGN_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign
      - TTS_WARMUP_ON_START=false
      - NVIDIA_VISIBLE_DEVICES=0,2
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      # Mount source for live edits (no rebuild)
      - ./:/app
      # Mount model cache for persistence
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0', '2' ]
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:19160/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GPU-enabled service with vLLM-Omni backend
  qwen3-tts-vllm:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: qwen3-tts-api-vllm
    network_mode: host
    ports:
      - "19160:19160"
    environment:
      - HOST=0.0.0.0
      - PORT=19160
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=vllm_omni
      - TTS_WARMUP_ON_START=true
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-Base
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/root/.cache/huggingface
    volumes:
      # Mount source for live edits (no rebuild)
      - ./:/app
      # Mount model cache for persistence
      - ~/.cache/huggingface:/root/.cache/huggingface
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '2' ]
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:19160/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - vllm

  # CPU-only service
  qwen3-tts-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-base
    container_name: qwen3-tts-api-cpu
    network_mode: host
    ports:
      - "19160:19160"
    environment:
      - HOST=0.0.0.0
      - PORT=19160
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-Base
      - TTS_MULTI_MODEL=true
      - TTS_SPEECH_MODEL_NAME=none
      - TTS_BASE_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-Base
      - TTS_VOICE_DESIGN_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign
    volumes:
      # Mount source for live edits (no rebuild)
      - ./:/app
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:19160/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - cpu

# To run GPU version with official backend: docker-compose up qwen3-tts-gpu
# To run GPU version with vLLM backend: docker-compose --profile vllm up qwen3-tts-vllm
# To run CPU version: docker-compose --profile cpu up qwen3-tts-cpu
